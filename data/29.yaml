#
# 29.yaml | PythonAnywhere
# https://blog.pythonanywhere.com/189/
#
# A critical storage volume failed. "It was caused by an extremely unlikely storage system failure, but despite that it should not have led to such a lengthy downtime, and should not have affected so many people."
#
# Researcher Notes:
#
# - Added the wishy-washy AIs, as they are interesting. Actually, the most interesting thing is that even at the time the IR is published, they were unsure what AIs to do (and how). Apparently there are tradeoffs and difficulties with avoiding these kinds of incidents.

IR-29:
  extractedBy: Matt
  reviewedBy: js

  ######################################
  # Meta
  ######################################

  patterns:
    - Across-Incident-Life-Cycle / Responder options:
      - AI-2 diagnosis tooling 
      - AI-3 recovery tooling (restore data)
      - AI-4 recovery tooling (warmup volumes)

  memos:
    - AI-Section-Title: Section called "Next steps"
    - Cautious Evolution: Everything couched in a perhaps we can do this

    # We think that instead it’s more productive to accept that from time to time, storage will fail, however redundant it is. What we need to do is handle the problem more gracefully and faster.
    - Framing the work: The nature of the AIs follow from the perspective that the failure can not (effectively) be prevented, as there are practical limits to storage hardware redundancy. So they focus not on preventing the triggering event, but handling such events "more gracefully and faster". AIs cover analysability, recoverability and fault tolerance.

    # The first thing to note here is that storage volume failure of this kind is incredibly rare [...] something with a 0.1% - 0.2% chance of happening in any given year [...]
    - Similar and rare failures: The relationship between the probability of recurrence and the AIs is interesting. In this case, they expect to see such a failure about once every 20 years. Put another way, the probability of a similar failure (well similar along this dimension) is rare, which in part may explain why the AIs are wishy-washy (see more below).

    # We have some plans on what our next steps should be, and will be implementing at least some of them over the coming months.
    - Uncertain proposals: This incident emphasizes that AIs are *proposed* action items. Some will take months and others may never happen ("will be implementing at least some of them over the coming months"). Also, because of the tradeoffs and risks involved some additional "thought is needed", some AIs they "would need to test and profile that carefully before doing it", and some are "hard to sort out". So for these multiple reasons they are far from committed to the listed AIs.

    # The downside is that that means it’s not something we’ve seen before – in our live environment, our staging environment, or in development.
    - The more rare, the less well prepared a system might be perhaps

  ######################################
  # AIs
  ######################################

  AI-1:
    # this outage affected pretty much everyone who uses our site at some point, and that is a problem. We need to work on ways to improve things so that if one file server fails, the effects are limited to people whose data is stored on that server [...] 

    motivation:
      experience: Storage volume failure propagated to affect more than just the files on the volume, as multiple types of workers servicing all traffic and background processes were blocked
      lifecycle:
        - Failure / Downstream Effects
      tags:
        - infrastructure disruption (storage volume failure)
        - propagation (effects from dependency via shared worker processes)
      iso:
        - Fault tolerance
        - Co-existence

    item:
      action: Find ways to improve handling of single server failure
      target: Storage system (filesystem) and dependents
      goal: Limit server failure impact to direct (data) dependents

      action-type:
        - formative / discover options
      target-type:
        - Data stores / File server
      target-sys: interaction
      sts:
        - UNCLEAR
      intended-effect:
        - Reliability / Fault tolerance (limit impact)
      temporal: new
      google-ai:
        - mitigate future #/ limit scope of impact

    other:
      modification: Improve isolation of failure
      level: system
      timing: plan and do / xN
      qa-category: contain failure

    themes:
      - goal not plan

    memos:
      - Commitment: Wishy-washy language

  AI-2:
    # Identifying the underlying cause of the problem. [...] We need to think hard to see if we can find a way to do this, however; while we don’t have the AWS team’s admin tools, we do have root access to the machine, so in theory there should be a way to find out if a particular volume is hanging.

    motivation:
      experience: There were challenges to discovering the underlying cause of the problem, which was hardware-related
      lifecycle:
        - Response / Diagnosis
      tags:
        - delayed RCA (complex hardware issues and relying on 3rd party to manage infrastructure)
        - complexity
        - human factors (missing expertise)
      iso:
        - Recoverability
        - Analysability

    item:
      action: Find ways to determine state of storage volume
      target: Storage tooling (diagnosing volumes from host)
      goal: Speed up diagnosis of hung storage volume

      action-type:
        - formative / discover options
      target-type:
        - Data stores / Metrics
      target-sys: element
      sts:
        - Tech / Support
      intended-effect:
        - Maintainability / Analysability
      temporal: new
      google-ai:
        - mitigate future #/ diagnosis (speed up)

    other:
      level: component
      timing: plan and do / x1
      qa-category: visibility

    themes:
      - goal not plan
      - tooling for diagnosis

    memos:
      - Commitment: Wishy-washy language

  AI-3:
    # Rebuilding a file server’s volumes from its backup server. In retrospect, we were right to do this rather than to promote the backup server to being the primary server. However, because we were putting scripts together (a combination of our normal backup scripts and the tools we use to recover backups when people accidentally delete files) in realtime, it took almost an hour. This is something we should have ready to go in a fully tested script so that we just need to run something like python3 replace_file_server_storage_with_backups.py livefile1

    motivation:
      experience: They had to build recovery scripts on the fly to restore the volumes from the backup server which took an hour
      lifecycle:
        - Response / Mitigation
      tags:
        - delayed RCA (missing tools and scripts to restore from backup server)
      iso:
        - Recoverability
        - Operability

    item:
      action: Create and test script to automate storage volume rebuild from backup
      target: Storage tooling (volume management scripts)
      goal: Speed up storage volume rebuild (and make more reliable)

      action-type:
        - evo / UNCLEAR
      target-type:
        - Data stores / Backup and recovery
      target-sys: element
      sts:
        - Tech / Support
      intended-effect:
        - Reliability / Recoverability (automated)
        - Interaction Capability / Operability (responder actions)
      temporal: new
      google-ai:
        - mitigate future #/ recovery (speed up)

    other:
      modification: Manual rebuild operation (fully tested script)
      timing: future
      qa-category: recovering to safe range

    themes:
      - writing tooling during response
      - operator options / recovery actions 

    memos:
      - Commitment: Wishy-washy language

  AI-4:
    # Warming up the new volumes. This one’s pretty hard to sort out [...] One possibility would be to shut down the backup server, take the snapshots, but then put the old backup volumes on the file server, and start up the backup server with the unwarmed volumes that were created from the snapshots. That should mean that the file storage would be warmed up as soon as it came into use, and would just slow the initial sync between the servers down somewhat. We would need to test and profile that carefully before doing it, though. 

    motivation:
      experience: New volumes copy over data upon read, which means that they start off quite slow.
      lifecycle:
        - Failure / Recovery
      tags:
        - delayed recovery (continued latency effects after some time)
        - slow operation
        - backup issues
      iso:
        - Recoverability

    item:
      action: Develop & test & profile alternative volume rebuild approaches
      target: Storage backup & replacement procedures for volumes
      goal: Speed up initialization of new storage volumes

      action-type:
        - evo / UNCLEAR
      target-type:
        - Data stores / Backup and recovery
      target-sts: process
      target-sys: element
      sts:
        - Process / Recovery process
      intended-effect:
        - Reliability / Recoverability
      temporal: new
      google-ai:
        - mitigate future #/ recovery # was slow even after rebuild
      
    other:
      modification: Define replace volume operation (automate)
      activity: design, develop, test, profile
      timing: use next time / test 1st
      qa-category: recovering to safe range 

    themes:
      - responder options / recovery time and risks # "leaving us open to problems from another volume failure"
      - possible action ("One possibility would be to ...")
      - half-recovered state 

    memos:
      - Commitment: Wishy-washy language
      - Acceptable-Risk: When repair/evolution occurs leaves you open to existing problem.